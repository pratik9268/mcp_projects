{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288901fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.github import GithubFileLoader\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2ea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "github_token = os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f17d8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GithubFileLoader\n  Value error, Did not find access_token, please add an environment variable `GITHUB_PERSONAL_ACCESS_TOKEN` which contains it, or pass `access_token` as a named parameter. [type=value_error, input_value={'repo': 'pratik9268/mcp_... at 0x000001CABA063060>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m loader = \u001b[43mGithubFileLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpratik9268/mcp_projects\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgithub_api_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://api.github.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgithub_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure this variable is defined\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfile_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecipe_reccomendation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\corp8.ai internship\\my_env\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for GithubFileLoader\n  Value error, Did not find access_token, please add an environment variable `GITHUB_PERSONAL_ACCESS_TOKEN` which contains it, or pass `access_token` as a named parameter. [type=value_error, input_value={'repo': 'pratik9268/mcp_... at 0x000001CABA063060>}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "loader = GithubFileLoader(\n",
    "    repo=\"pratik9268/mcp_projects\",\n",
    "    branch=\"main\",\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    access_token=github_token,  # Ensure this variable is defined\n",
    "    file_filter=lambda file_path: \"recipe_reccomendation\" in file_path and file_path.endswith(\".py\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe02081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1735ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema import SystemMessage, HumanMessage\n",
      "import pandas as pd\n",
      "import os\n",
      "\n",
      "# Set your OpenAI API key\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"  # Replace with your actual key\n",
      "\n",
      "# URL to scrape\n",
      "url = \"\"\n",
      "response = requests.get(url)\n",
      "soup = BeautifulSoup(response.text, \"html.parser\")\n",
      "\n",
      "# Extract raw text content\n",
      "text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
      "\n",
      "# Define the system prompt\n",
      "system_msg = SystemMessage(content=\"\"\"\n",
      "You are a travel assistant AI. Extract and summarize the following details from the text:\n",
      "- name (title of the package)\n",
      "- category (Adventure, Nature, Heritage, Spiritual, etc.)\n",
      "- highlights (top 3 attractions or experiences)\n",
      "- duration (approximate in days)\n",
      "- price (approximate in INR)\n",
      "Return it as CSV line: name,category,highlights,duration,price\n",
      "\"\"\")\n",
      "\n",
      "# Ask model to extract info\n",
      "human_msg = HumanMessage(content=text_content)\n",
      "\n",
      "# Initialize LLM\n",
      "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "\n",
      "# Call the model\n",
      "response = llm([system_msg, human_msg])\n",
      "\n",
      "# Parse response\n",
      "print(\"Extracted CSV Line:\\n\", response.content)\n",
      "\n",
      "# Save as CSV\n",
      "columns = [\"name\", \"category\", \"highlights\", \"duration\", \"price\"]\n",
      "values = response.content.strip().split(\",\")\n",
      "\n",
      "df = pd.DataFrame([values], columns=columns)\n",
      "df.to_csv(\"langchain_tour_output.csv\", index=False)\n",
      "print(\"Saved to langchain_tour_output.csv\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a2bf27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'd:\\\\corp8.ai internship\\\\mcp_projects\\\\github_QA_chatbot\\\\cloned_repo\\\\.git'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo_url = \"https://github.com/pratik9268/mcp_projects.git\"\n",
    "clone_dir = \"cloned_repo\"\n",
    "\n",
    "Repo.clone_from(repo_url, clone_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bbd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_dir = \"cloned_repo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b1157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=clone_dir,\n",
    "    glob=\"**/*.py\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a3c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import streamlit as st\n",
      "import asyncio\n",
      "from fastmcp import Client\n",
      "\n",
      "# Initialize MCP client with your MCP server file path or URL\n",
      "client = Client(\"mcp_server.py\")\n",
      "\n",
      "query = \"sentiment analysis\"\n",
      "\n",
      "async def query_agent(query):\n",
      "    async with client:\n",
      "        return await client.call_tool(\"query_papers\", {\"query\": query})\n",
      "\n",
      "result = asyncio.run(query_agent(query))\n",
      "\n",
      "print(type(result))\n",
      "print(result[0].text)\n",
      "# st.title(\"Arxiv Paper Finder\")\n",
      "\n",
      "# query = st.text_input(\"Enter your search query:\")\n",
      "\n",
      "# if st.button(\"Search\"):\n",
      "#     if query.strip():\n",
      "#         with st.spinner(\"Fetching papers...\"):\n",
      "#             result = asyncio.run(query_agent(query))\n",
      "#             st.markdown(\"### Results:\")\n",
      "#             st.markdown(result[0].text)  # format line breaks nicely\n",
      "#     else:\n",
      "#         st.warning(\"Please enter a query.\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,Language\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=120,\n",
    ")\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0033b6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fe20efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent\n",
      "from langchain_core.tools import tool\n",
      "from dotenv import load_dotenv\n",
      "import arxiv\n",
      "\n",
      "# Load environment\n",
      "load_dotenv()\n",
      "\n",
      "# Define the tool function\n",
      "@tool\n"
     ]
    }
   ],
   "source": [
    "print(chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14ae6b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_papers(query: str):\n",
      "    \"\"\"Fetch 3 relevant research papers from arXiv for a given query.\"\"\"\n",
      "    client = arxiv.Client()\n",
      "    search = arxiv.Search(\n",
      "        query=query,\n",
      "        max_results=3,\n",
      "        sort_by=arxiv.SortCriterion.Relevance\n",
      "    )\n",
      "    results = []\n",
      "    for result in client.results(search):\n",
      "        results.append({\n",
      "            \"title\": result.title,\n",
      "            \"summary\": result.summary,\n",
      "            \"authors\": [author.name for author in result.authors],\n",
      "            \"links\": result.pdf_url\n",
      "        })\n",
      "    return results\n",
      "tools = [get_papers]\n",
      "# Initialize LLM\n"
     ]
    }
   ],
   "source": [
    "print(chunks[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc138cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741c8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01404782",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    "    model_type=\"passage\",\n",
    "    api_key=nvidia_api_key\n",
    ")\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    embedding = embeddings,\n",
    "    documents= chunks,\n",
    "    persist_directory='github_chroma_db',\n",
    "    collection_name='sample'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbf47de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334b47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "710b3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also don't mix candidate home address detail with candidates education address details.\n",
      "        while providing the updated ressume based on suggestion and feedback do not change the fromat of resume. you can only suggest to change formate if you feel according to resume.\n",
      "        and also check that given {page_content} is a resume or not. if it is any other documents data then give response only like i can only analyze resume please upload resume and not give any feedback and suggestions.\n",
      "    \"\"\",\n",
      "def analyze_resume(page_content : str):\n",
      "    llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
      "page_content = docs[0].page_content\n",
      "\n",
      "    async def resume_agent(page_content):\n",
      "        async with client:\n",
      "            return await client.call_tool(\"call_analyze_resume\", {\"page_content\": page_content})\n",
      "    \n",
      "    with st.spinner(\"Analyzing your resume...\"):\n",
      "also don't mix candidate home address detail with candidates education address details.\n",
      "    while providing the updated ressume based on suggestion and feedback do not change the fromat of resume. you can only suggest to change formate if you feel according to resume.\n",
      "\"\"\",\n",
      "promt_template = PromptTemplate(\n",
      "    template= \"\"\"\n",
      "    you are a helpful assistant which provides feedback and improvements to user resume.\n",
      "    for that u you will get content of user's resume as {page_content}.\n",
      "    your response will be improvements and feedback to user regarding their resume.\n",
      "    In experinece section if address is added that will be the address of the company so note that point don't consider that address as candidate's.\n",
      "    also don't mix candidate home address detail with candidates education address details.\n",
      "promt_template = PromptTemplate(\n",
      "        template= \"\"\"\n",
      "        you are a helpful assistant which provides feedback and improvements to user resume.\n",
      "        for that u you will get content of user's resume as {page_content}.\n",
      "        your response will be improvements and feedback to user regarding their resume.\n",
      "        In experinece section if address is added that will be the address of the company so note that point don't consider that address as candidate's.\n",
      "        also don't mix candidate home address detail with candidates education address details.\n",
      "def call_analyze_resume(page_content: str):\n",
      "    \"\"\"Use LangChain agent to query papers.\"\"\"\n",
      "    print(\"calling anlyze_resume\")\n",
      "    result = analyze_resume(page_content)\n",
      "    return result\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run(transport=\"streamable-http\")\n",
      "    # mcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", port=9000)\n",
      "    # mcp.run(\n",
      "    #     transport=\"sse\",\n",
      "    #     host=\"127.0.0.1\",\n",
      "    #     port=4200,\n",
      "    #     log_level=\"debug\",\n",
      "    #     path=\"/my-custom-sse-path\",\n",
      "    # )\n",
      "from fastmcp import FastMCP\n",
      "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "mcp = FastMCP('resume_analyzer_server')\n",
      "# File upload\n",
      "uploaded_file = st.file_uploader(\"Upload your resume (PDF only)\", type=[\"pdf\"])\n",
      "\n",
      "if uploaded_file is not None:\n",
      "    # Save uploaded file\n",
      "    pdf_path = os.path.join(\"temp\", uploaded_file.name)\n",
      "    os.makedirs(\"temp\", exist_ok=True)\n",
      "    with open(pdf_path, \"wb\") as f:\n",
      "        f.write(uploaded_file.getbuffer())\n",
      "    st.success(\"Document uploaded successfully!\")\n",
      "\n",
      "    # Load resume content\n",
      "    loader = PyPDFLoader(pdf_path)\n",
      "    docs = loader.load()\n",
      "\n",
      "    page_content = docs[0].page_content\n",
      "Format:\n",
      "\n",
      "Paper 1:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "\n",
      "Paper 2:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "\n",
      "Paper 3:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "your final response will be the format not any other things like what you think, what input you provide to tool, etc\n",
      "\n",
      "{agent_scratchpad}\n",
      "\"\"\"\n",
      ")\n",
      "\n",
      "\n",
      "agent = create_tool_calling_agent(\n",
      "    llm=llm,\n",
      "    tools=[get_papers],\n",
      "    prompt=prompt\n",
      ")\n",
      "\n",
      "agent_executor = AgentExecutor(\n",
      "    agent=agent,\n",
      "    tools=[get_papers],\n",
      "    # verbose=True,\n",
      "    handle_parsing_errors=True\n",
      ")\n",
      "\n",
      "response = agent_executor.invoke({\"query\": \"sentiment analysis\"})\n",
      "# print(response)\n",
      "import streamlit as st\n",
      "import os\n",
      "import asyncio\n",
      "from fastmcp import Client\n",
      "from langchain_community.document_loaders import PyPDFLoader\n",
      "\n",
      "client = Client(\"http://127.0.0.1:8000/mcp\")\n",
      "\n",
      "# Title\n",
      "st.title(\"üìÑ Resume Feedback and suggestion\")\n",
      "\n",
      "# File upload\n",
      "uploaded_file = st.file_uploader(\"Upload your resume (PDF only)\", type=[\"pdf\"])\n",
      "print(\"calling resume_agent\")\n",
      "        result = asyncio.run(resume_agent(page_content))\n",
      "        print(\"get response from server\")\n",
      "        st.subheader(\"üìù Resume Feedback:\")\n",
      "        st.write(result[0].text\n",
      "        \n",
      "        )\n",
      "    # Cleanup: remove the uploaded PDF file\n",
      "    try:\n",
      "        os.remove(pdf_path)\n",
      "        print(\"Temporary file removed.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error removing temp file: {e}\")\n",
      "response = agent_executor.invoke({\"query\": \"sentiment analysis\"})\n",
      "# print(response)\n",
      "\n",
      "print(response['output'])\n",
      "from langchain_community.document_loaders import PyPDFLoader\n",
      "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
      "from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "\n",
      "pdf_path = 'pratik_resume.pdf'\n",
      "loader = PyPDFLoader(pdf_path)\n",
      "docs = loader.load()\n",
      "print(type(docs[0].page_content))\n",
      "docs\n",
      "\n",
      "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
      "Your job is to find the top 3 relevant papers according to the user query: {query},\n",
      "first understand the user query that what kind of papers user is looking for and make needful input from query to provide the tool 'get_papers'\n",
      "note that you will only use the tool 'get_papers' to find the papers, you can't find by your self and tool finds paper from arxiv not other website.\n",
      "The input to the tool will be a string query.\n",
      "The tool will return a response in the form of a list of dictionaries.\n",
      "Each dictionary includes: title, authors, summary, and links.\n",
      "Your job is to find the top 3 relevant papers according to the user query: {query},\n",
      "first understand the user query that what kind of papers user is looking for and make needful input from query to provide the tool 'get_papers'\n",
      "note that you will only use the tool 'get_papers' to find the papers, you can't find by your self and tool finds paper from arxiv not other website.\n",
      "The input to the tool will be a string query.\n",
      "The tool will return a response in the form of a list of dictionaries.\n",
      "Each dictionary includes: title, authors, summary, and links.\n",
      "Format:-\n",
      "\n",
      "Paper 1:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "\n",
      "Paper 2:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "\n",
      "Paper 3:\n",
      "\n",
      "Title:\n",
      "Authors:\n",
      "Summary:\n",
      "Links:\n",
      "\n",
      "your final response will be the format not any other things like what you think, what input you provide to tool, etc\n",
      "\n",
      "{agent_scratchpad}\n",
      "\"\"\"\n",
      ")\n",
      "\n",
      "agent = create_tool_calling_agent(\n",
      "    llm=llm,\n",
      "    tools=tools,\n",
      "    prompt=prompt,\n",
      ")\n",
      "\n",
      "agent_executor = AgentExecutor(\n",
      "    agent=agent,\n",
      "    tools=tools,\n",
      "    handle_parsing_errors=True\n",
      ")\n",
      "\n",
      "# Expose agent as MCP tool\n",
      "@mcp.tool()\n",
      "Each dictionary includes: title, authors, summary, and links.\n",
      "You must always return the response in the following format:\n",
      "Each dictionary includes: title, authors, summary, and links.\n",
      "You must always return the response in the following format:\n",
      "def get_papers(query: str):\n",
      "    \"\"\"Fetch 3 relevant research papers from arXiv for a given query.\"\"\"\n",
      "    client = arxiv.Client()\n",
      "    search = arxiv.Search(\n",
      "        query=query,\n",
      "        max_results=3,\n",
      "        sort_by=arxiv.SortCriterion.Relevance\n",
      "    )\n",
      "    results = []\n",
      "    for result in client.results(search):\n",
      "        results.append({\n",
      "            \"title\": result.title,\n",
      "            \"summary\": result.summary,\n",
      "            \"authors\": [author.name for author in result.authors],\n",
      "            \"links\": result.pdf_url\n",
      "        })\n",
      "    return results\n",
      "tools = [get_papers]\n",
      "# Initialize LLM\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"query\", \"agent_scratchpad\"],  # removed \"tools\"\n",
      "    template=\"\"\"\n",
      "You are a helpful ReAct agent that uses the following tools:\n",
      "[get_papers]\n",
      "import streamlit as st\n",
      "import asyncio\n",
      "from fastmcp import Client\n",
      "\n",
      "# Initialize MCP client with your MCP server file path or URL\n",
      "client = Client(\"mcp_server.py\")\n",
      "\n",
      "query = \"sentiment analysis\"\n",
      "\n",
      "async def query_agent(query):\n",
      "    async with client:\n",
      "        return await client.call_tool(\"query_papers\", {\"query\": query})\n",
      "\n",
      "result = asyncio.run(query_agent(query))\n",
      "\n",
      "print(type(result))\n",
      "print(result[0].text)\n",
      "# st.title(\"Arxiv Paper Finder\")\n",
      "\n",
      "# query = st.text_input(\"Enter your search query:\")\n",
      "\"\"\",\n",
      "        input_variables = ['context', 'ingridents']\n",
      "    )\n",
      "\n",
      "    parser = StrOutputParser()\n",
      "\n",
      "    def format_docs(retrieved_docs):\n",
      "        context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
      "        return context_text\n",
      "\n",
      "    parallel_chain = RunnableParallel({\n",
      "        'context': retriever | RunnableLambda(format_docs),\n",
      "        'ingridents': RunnablePassthrough()\n",
      "    })\n",
      "\n",
      "    main_chain = parallel_chain | prompt | llm | parser\n",
      "\n",
      "    response = main_chain.invoke(ingridents)\n",
      "    return response\n",
      "\n",
      "\n",
      "@mcp.tool()\n",
      "def get_papers(query: str):\n",
      "    \"\"\"Fetch 3 relevant research papers from arXiv for a given query.\"\"\"\n",
      "    client = arxiv.Client()\n",
      "    search = arxiv.Search(\n",
      "        query=query,\n",
      "        max_results=3,\n",
      "        sort_by=arxiv.SortCriterion.Relevance\n",
      "    )\n",
      "    results = []\n",
      "    for result in client.results(search):\n",
      "        results.append({\n",
      "            \"title\": result.title,\n",
      "            \"summary\": result.summary,\n",
      "            \"authors\": [author.name for author in result.authors],\n",
      "            \"links\": result.pdf_url\n",
      "        })\n",
      "    return results\n",
      "# query = st.text_input(\"Enter your search query:\")\n",
      "\n",
      "# if st.button(\"Search\"):\n",
      "#     if query.strip():\n",
      "#         with st.spinner(\"Fetching papers...\"):\n",
      "#             result = asyncio.run(query_agent(query))\n",
      "#             st.markdown(\"### Results:\")\n",
      "#             st.markdown(result[0].text)  # format line breaks nicely\n",
      "#     else:\n",
      "#         st.warning(\"Please enter a query.\")\n",
      "def query_papers(query: str):\n",
      "    \"\"\"Use LangChain agent to query papers.\"\"\"\n",
      "    result = agent_executor.invoke({\"query\": query})\n",
      "    return result[\"output\"]\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    mcp.run()\n",
      "# Setup the LangChain agent tool using the above get_papers tool\n",
      "tools = [get_papers]\n",
      "\n",
      "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
      "\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"query\", \"agent_scratchpad\"],  # removed \"tools\"\n",
      "    template=\"\"\"\n",
      "You are a helpful ReAct agent that uses the following tools:\n",
      "[get_papers]\n",
      "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
      "\n",
      "    prompt = PromptTemplate(\n",
      "        template=\"\"\"\n",
      "        You are a helpful culinary assistant that suggests recipes based on the ingredients a user has. \n",
      "    Your task is to recommend 3-5 relevant recipes from the provided context.\n",
      "user will provide only ingridents that they have and you need to give them top 3 or 5 food recipe from the context relevant to the user ingridents.\n",
      "      In your response you will provide title of that recipe, ingridents to make that food, how to make it (direction).\n",
      "      the direction you provide has to be step wise and named is as steps not direction.\n",
      "\n",
      "      If the context is insufficient, just say you don't know.\n",
      "\n",
      "      {context}\n",
      "      Ingriidents: {ingridents}\n",
      "    \"\"\",\n",
      "    input_variables = ['context', 'ingridents']\n",
      ")\n",
      "\n",
      "parser = StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "for i in range(29):\n",
    "    print(retriever.invoke('tell me about resume analyzing')[i].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20c4d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "444f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64c44a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "     You are an expert technical assistant for a GitHub repository. Follow these rules strictly:\n",
    "\n",
    "1. When the user asks a general question about the codebase:\n",
    "- Provide a clear, concise explanation\n",
    "- Do NOT include code snippets unless explicitly asked\n",
    "- Offer to show code if it would be helpful\n",
    "\n",
    "2. Only when the user uses phrases like:\n",
    "- \"Show me the code for...\"\n",
    "- \"Provide a code snippet of...\"\n",
    "- \"Display the implementation of...\"\n",
    "- \"Can I see the actual code...\"\n",
    "- Explicitly requests code with words like \"code\", \"snippet\", \"implementation\"\n",
    "\n",
    "Then:\n",
    "- Provide the relevant code in a markdown block with proper syntax highlighting\n",
    "- Include the exact file path\n",
    "- Add a brief explanation before/after the code\n",
    "\n",
    "3. For all responses:\n",
    "- Be technically accurate\n",
    "- Keep explanations clear and concise\n",
    "- Reference architectural components when helpful\n",
    "\n",
    "Current repository context:\n",
    "{context}\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Answer in markdown format:\n",
    "    \"\"\",\n",
    "    input_variables = ['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5416be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke('give me idea about this repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b1a859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a46c5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'give me idea about this repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16f825ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.invoke({\"context\": context_text, \"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fcbec9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n     You are an expert technical assistant for a GitHub repository. Follow these rules strictly:\\n\\n1. When the user asks a general question about the codebase:\\n- Provide a clear, concise explanation\\n- Do NOT include code snippets unless explicitly asked\\n- Offer to show code if it would be helpful\\n\\n2. Only when the user uses phrases like:\\n- \"Show me the code for...\"\\n- \"Provide a code snippet of...\"\\n- \"Display the implementation of...\"\\n- \"Can I see the actual code...\"\\n- Explicitly requests code with words like \"code\", \"snippet\", \"implementation\"\\n\\nThen:\\n- Provide the relevant code in a markdown block with proper syntax highlighting\\n- Include the exact file path\\n- Add a brief explanation before/after the code\\n\\n3. For all responses:\\n- Be technically accurate\\n- Keep explanations clear and concise\\n- Reference architectural components when helpful\\n\\nCurrent repository context:\\nimport os\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\nfrom dotenv import load_dotenv\\nfrom langchain_chroma import Chroma\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\nnvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\\n\\nimport os\\nfrom fastmcp import FastMCP\\nfrom langchain_chroma import Chroma\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA,NVIDIAEmbeddings\\nfrom langchain_core.prompts import PromptTemplate\\nfrom dotenv import load_dotenv\\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\\nfrom langchain_core.output_parsers import StrOutputParser\\n\\nload_dotenv()\\nnvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\\n\\nmcp = FastMCP(\\'recipe_chatbot_server\\')\\n\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\n    prompt = PromptTemplate(\\n        template=\"\"\"\\n        You are a helpful culinary assistant that suggests recipes based on the ingredients a user has. \\n    Your task is to recommend 3-5 relevant recipes from the provided context.\\n\\nload_dotenv()\\nnvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")\\n\\n# Reinitialize embeddings (must match the one used to store vectors)\\nembeddings = NVIDIAEmbeddings(\\n    model=\"nvidia/nv-embedqa-e5-v5\",\\n    model_type=\"passage\",\\n    api_key=nvidia_api_key\\n)\\n\\n# Load existing vector store from disk\\nvector_store = Chroma(\\n    persist_directory=\\'my_chroma_db\\',\\n    embedding_function=embeddings,\\n    collection_name=\\'sample\\'\\n)\\n\\nretriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\\n\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nprompt = PromptTemplate(\\n    template=\"\"\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided context.\\n      Context is about food recipe which includes title, direction, ingridients and link to the website of that food recipe.\\n      title is the name of the food recipe.\\n      direction describes the steps or process of making that food.\\n      ingridents describes that what are the things required to make that food.\\n\\npromt_template = PromptTemplate(\\n    template= \"\"\"\\n    you are a helpful assistant which provides feedback and improvements to user resume.\\n    for that u you will get content of user\\'s resume as {page_content}.\\n    your response will be improvements and feedback to user regarding their resume.\\n    In experinece section if address is added that will be the address of the company so note that point don\\'t consider that address as candidate\\'s.\\n    also don\\'t mix candidate home address detail with candidates education address details.\\n\\npromt_template = PromptTemplate(\\n        template= \"\"\"\\n        you are a helpful assistant which provides feedback and improvements to user resume.\\n        for that u you will get content of user\\'s resume as {page_content}.\\n        your response will be improvements and feedback to user regarding their resume.\\n        In experinece section if address is added that will be the address of the company so note that point don\\'t consider that address as candidate\\'s.\\n        also don\\'t mix candidate home address detail with candidates education address details.\\n\\nfrom langchain_community.document_loaders import PyPDFLoader\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain_core.prompts import PromptTemplate\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\npdf_path = \\'pratik_resume.pdf\\'\\nloader = PyPDFLoader(pdf_path)\\ndocs = loader.load()\\nprint(type(docs[0].page_content))\\ndocs\\n\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nresponse = agent_executor.invoke({\"query\": \"sentiment analysis\"})\\n# print(response)\\n\\nprint(response[\\'output\\'])\\n\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain.agents import create_react_agent, AgentExecutor, create_tool_calling_agent\\nfrom langchain_core.tools import tool\\nfrom dotenv import load_dotenv\\nimport arxiv\\n\\n# Load environment\\nload_dotenv()\\n\\n# Define the tool function\\n@tool\\n\\nalso don\\'t mix candidate home address detail with candidates education address details.\\n        while providing the updated ressume based on suggestion and feedback do not change the fromat of resume. you can only suggest to change formate if you feel according to resume.\\n        and also check that given {page_content} is a resume or not. if it is any other documents data then give response only like i can only analyze resume please upload resume and not give any feedback and suggestions.\\n    \"\"\",\\n\\ndef get_papers(query: str):\\n    \"\"\"Fetch 3 relevant research papers from arXiv for a given query.\"\"\"\\n    client = arxiv.Client()\\n    search = arxiv.Search(\\n        query=query,\\n        max_results=3,\\n        sort_by=arxiv.SortCriterion.Relevance\\n    )\\n    results = []\\n    for result in client.results(search):\\n        results.append({\\n            \"title\": result.title,\\n            \"summary\": result.summary,\\n            \"authors\": [author.name for author in result.authors],\\n            \"links\": result.pdf_url\\n        })\\n    return results\\ntools = [get_papers]\\n# Initialize LLM\\n\\ndef recipe_finder(ingridents : str):\\n    # Reinitialize embeddings (must match the one used to store vectors)\\n    embeddings = NVIDIAEmbeddings(\\n        model=\"nvidia/nv-embedqa-e5-v5\",\\n        model_type=\"passage\",\\n        api_key=nvidia_api_key\\n    )\\n\\n    # Load existing vector store from disk\\n    vector_store = Chroma(\\n        persist_directory=\\'my_chroma_db\\',\\n        embedding_function=embeddings,\\n        collection_name=\\'sample\\'\\n    )\\n\\n    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 30})\\n\\n    llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nfrom fastmcp import FastMCP\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain_core.prompts import PromptTemplate\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nmcp = FastMCP(\\'resume_analyzer_server\\')\\n\\nfrom fastmcp import FastMCP\\nimport arxiv\\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\\nfrom langchain_core.tools import tool\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\n\\nmcp = FastMCP(name=\"arxiv_server\")\\n\\n@tool()\\n\\ndef get_papers(query: str):\\n    \"\"\"Fetch 3 relevant research papers from arXiv for a given query.\"\"\"\\n    client = arxiv.Client()\\n    search = arxiv.Search(\\n        query=query,\\n        max_results=3,\\n        sort_by=arxiv.SortCriterion.Relevance\\n    )\\n    results = []\\n    for result in client.results(search):\\n        results.append({\\n            \"title\": result.title,\\n            \"summary\": result.summary,\\n            \"authors\": [author.name for author in result.authors],\\n            \"links\": result.pdf_url\\n        })\\n    return results\\n\\nYour job is to find the top 3 relevant papers according to the user query: {query},\\nfirst understand the user query that what kind of papers user is looking for and make needful input from query to provide the tool \\'get_papers\\'\\nnote that you will only use the tool \\'get_papers\\' to find the papers, you can\\'t find by your self and tool finds paper from arxiv not other website.\\nThe input to the tool will be a string query.\\nThe tool will return a response in the form of a list of dictionaries.\\nEach dictionary includes: title, authors, summary, and links.\\n\\nYour job is to find the top 3 relevant papers according to the user query: {query},\\nfirst understand the user query that what kind of papers user is looking for and make needful input from query to provide the tool \\'get_papers\\'\\nnote that you will only use the tool \\'get_papers\\' to find the papers, you can\\'t find by your self and tool finds paper from arxiv not other website.\\nThe input to the tool will be a string query.\\nThe tool will return a response in the form of a list of dictionaries.\\nEach dictionary includes: title, authors, summary, and links.\\n\\nFormat:\\n\\nPaper 1:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\n\\nPaper 2:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\n\\nPaper 3:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\nyour final response will be the format not any other things like what you think, what input you provide to tool, etc\\n\\n{agent_scratchpad}\\n\"\"\"\\n)\\n\\n\\nagent = create_tool_calling_agent(\\n    llm=llm,\\n    tools=[get_papers],\\n    prompt=prompt\\n)\\n\\nagent_executor = AgentExecutor(\\n    agent=agent,\\n    tools=[get_papers],\\n    # verbose=True,\\n    handle_parsing_errors=True\\n)\\n\\nresponse = agent_executor.invoke({\"query\": \"sentiment analysis\"})\\n# print(response)\\n\\nimport streamlit as st\\nimport os\\nimport asyncio\\nfrom fastmcp import Client\\nfrom langchain_community.document_loaders import PyPDFLoader\\n\\nclient = Client(\"http://127.0.0.1:8000/mcp\")\\n\\n# Title\\nst.title(\"üìÑ Resume Feedback and suggestion\")\\n\\n# File upload\\nuploaded_file = st.file_uploader(\"Upload your resume (PDF only)\", type=[\"pdf\"])\\n\\ndef analyze_resume(page_content : str):\\n    llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nprint(\"calling resume_agent\")\\n        result = asyncio.run(resume_agent(page_content))\\n        print(\"get response from server\")\\n        st.subheader(\"üìù Resume Feedback:\")\\n        st.write(result[0].text\\n        \\n        )\\n    # Cleanup: remove the uploaded PDF file\\n    try:\\n        os.remove(pdf_path)\\n        print(\"Temporary file removed.\")\\n    except Exception as e:\\n        print(f\"Error removing temp file: {e}\")\\n\\nuser will provide only ingridents that they have and you need to give them top 3 or 5 food recipe from the context relevant to the user ingridents.\\n      In your response you will provide title of that recipe, ingridents to make that food, how to make it (direction).\\n      the direction you provide has to be step wise and named is as steps not direction.\\n\\n      If the context is insufficient, just say you don\\'t know.\\n\\n      {context}\\n      Ingriidents: {ingridents}\\n    \"\"\",\\n    input_variables = [\\'context\\', \\'ingridents\\']\\n)\\n\\nparser = StrOutputParser()\\n\\n\"\"\",\\n        input_variables = [\\'context\\', \\'ingridents\\']\\n    )\\n\\n    parser = StrOutputParser()\\n\\n    def format_docs(retrieved_docs):\\n        context_text = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n        return context_text\\n\\n    parallel_chain = RunnableParallel({\\n        \\'context\\': retriever | RunnableLambda(format_docs),\\n        \\'ingridents\\': RunnablePassthrough()\\n    })\\n\\n    main_chain = parallel_chain | prompt | llm | parser\\n\\n    response = main_chain.invoke(ingridents)\\n    return response\\n\\n\\n@mcp.tool()\\n\\nFormat:-\\n\\nPaper 1:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\n\\nPaper 2:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\n\\nPaper 3:\\n\\nTitle:\\nAuthors:\\nSummary:\\nLinks:\\n\\nyour final response will be the format not any other things like what you think, what input you provide to tool, etc\\n\\n{agent_scratchpad}\\n\"\"\"\\n)\\n\\nagent = create_tool_calling_agent(\\n    llm=llm,\\n    tools=tools,\\n    prompt=prompt,\\n)\\n\\nagent_executor = AgentExecutor(\\n    agent=agent,\\n    tools=tools,\\n    handle_parsing_errors=True\\n)\\n\\n# Expose agent as MCP tool\\n@mcp.tool()\\n\\nimport streamlit as st\\nimport os\\nimport asyncio\\nfrom fastmcp import Client\\n\\nclient = Client(\"http://127.0.0.1:8000/mcp\")\\n\\nst.set_page_config(page_title=\"Recipe Chatbot\", page_icon=\"üçΩÔ∏è\", layout=\"wide\")\\n\\n### Rules:\\n    1. Answer STRICTLY from the provided context only\\n    2. Context contains recipes with:\\n       - Title: Name of the recipe\\n       - Ingredients: Items needed to make the recipe\\n       - Directions: Preparation steps\\n       - Link: Source URL (do not include this in response)\\n    3. User will provide only ingredients they currently possess\\n    4. For each recommended recipe, provide:\\n       - Title\\n       - All required ingredients (not just what user has)\\n       - Step-by-step preparation instructions\\n    5. Format each recipe response as follows:\\n\\nprompt = PromptTemplate(\\n    input_variables=[\"query\", \"agent_scratchpad\"],  # removed \"tools\"\\n    template=\"\"\"\\nYou are a helpful ReAct agent that uses the following tools:\\n[get_papers]\\n\\nalso don\\'t mix candidate home address detail with candidates education address details.\\n    while providing the updated ressume based on suggestion and feedback do not change the fromat of resume. you can only suggest to change formate if you feel according to resume.\\n\"\"\",\\n\\ndef format_docs(retrieved_docs):\\n  context_text = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n  return context_text\\n\\nparallel_chain = RunnableParallel({\\n    \\'context\\': retriever | RunnableLambda(format_docs),\\n    \\'ingridents\\': RunnablePassthrough()\\n})\\n\\nmain_chain = parallel_chain | prompt | llm | parser\\n\\ni = input(\\'ingridents:\\')\\nanswer = main_chain.invoke(i)\\nprint(answer)\\n\\n# Setup the LangChain agent tool using the above get_papers tool\\ntools = [get_papers]\\n\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\nprompt = PromptTemplate(\\n    input_variables=[\"query\", \"agent_scratchpad\"],  # removed \"tools\"\\n    template=\"\"\"\\nYou are a helpful ReAct agent that uses the following tools:\\n[get_papers]\\n\\n\"links\": result.pdf_url\\n        })\\n    return results\\ntools = [get_papers]\\n# Initialize LLM\\nllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\\n\\ndef call_analyze_resume(page_content: str):\\n    \"\"\"Use LangChain agent to query papers.\"\"\"\\n    print(\"calling anlyze_resume\")\\n    result = analyze_resume(page_content)\\n    return result\\n\\nif __name__ == \"__main__\":\\n    mcp.run(transport=\"streamable-http\")\\n    # mcp.run(transport=\"streamable-http\", host=\"127.0.0.1\", port=9000)\\n    # mcp.run(\\n    #     transport=\"sse\",\\n    #     host=\"127.0.0.1\",\\n    #     port=4200,\\n    #     log_level=\"debug\",\\n    #     path=\"/my-custom-sse-path\",\\n    # )\\n\\n# File upload\\nuploaded_file = st.file_uploader(\"Upload your resume (PDF only)\", type=[\"pdf\"])\\n\\nif uploaded_file is not None:\\n    # Save uploaded file\\n    pdf_path = os.path.join(\"temp\", uploaded_file.name)\\n    os.makedirs(\"temp\", exist_ok=True)\\n    with open(pdf_path, \"wb\") as f:\\n        f.write(uploaded_file.getbuffer())\\n    st.success(\"Document uploaded successfully!\")\\n\\n    # Load resume content\\n    loader = PyPDFLoader(pdf_path)\\n    docs = loader.load()\\n\\n    page_content = docs[0].page_content\\n\\ndef query_papers(query: str):\\n    \"\"\"Use LangChain agent to query papers.\"\"\"\\n    result = agent_executor.invoke({\"query\": query})\\n    return result[\"output\"]\\n\\nif __name__ == \"__main__\":\\n    mcp.run()\\n\\nst.title(\"üç≥ AI Recipe Finder\")\\n\\nst.markdown(\\'<div class=\"subtitle\">Enter ingredients you have, and get recipes you can cook!</div>\\', unsafe_allow_html=True)\\n\\n# Custom big heading for text area\\nst.markdown(\\'<div class=\"custom-label\">Ingredients (comma-separated):</div>\\', unsafe_allow_html=True)\\n\\n# Now use empty label to avoid default text\\ningredients = st.text_area(\"\", height=200, key=\"ingredients\")\\n\\nasync def recipe_agent(ingredients):\\n    async with client:\\n        return await client.call_tool(\"call_recipe_finder\", {\"ingridents\": ingredients})\\n\\n### Important:\\n    - If no matching recipes exist in context, respond \"I couldn\\'t find matching recipes with your ingredients\"\\n    - Never invent recipes not present in the context\\n    - Ingredients list must be complete for each recipe\\n    - Directions must be numbered steps, not paragraphs\\n\\n    Context: {context}\\n    User\\'s Ingredients: {ingridents}\\n\\n        \"\"\",\\n        input_variables = [\\'context\\', \\'ingridents\\']\\n    )\\n\\n    parser = StrOutputParser()\\n\\npage_content = docs[0].page_content\\n\\n    async def resume_agent(page_content):\\n        async with client:\\n            return await client.call_tool(\"call_analyze_resume\", {\"page_content\": page_content})\\n    \\n    with st.spinner(\"Analyzing your resume...\"):\\n\\ndef call_recipe_finder(ingridents: str):\\n    print(\"calling recipe_finder\")\\n    result = recipe_finder(ingridents)\\n    return result\\n\\nif __name__ == \"__main__\":\\n    mcp.run(transport=\"streamable-http\")\\n\\ninput_variables=[\\'page_content\\']\\n    )\\n\\n    chain = promt_template | llm\\n    response = chain.invoke({\"page_content\":page_content})\\n    print(\"response generated\")\\n    return response.content\\n\\n@mcp.tool()\\n\\nUser question: give me idea about this repo\\n\\nAnswer in markdown format:\\n    ')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2dfe40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Overview of the Repository**\n",
      "\n",
      "This GitHub repository appears to be a collection of LangChain models and tools focused on natural language processing (NLP) tasks. LangChain is an open-source framework for building and deploying AI models. The repository contains various components, including:\n",
      "\n",
      "### Language Models\n",
      "\n",
      "* ChatNVIDIA: a language model from NVIDIA used for conversational AI tasks.\n",
      "* LLaMA: a large language model used for generating human-like text.\n",
      "\n",
      "### Tools and Agents\n",
      "\n",
      "* `get_papers`: a tool that fetches research papers from arXiv based on a user query.\n",
      "* `analyze_resume`: a tool that analyzes a user's resume and provides feedback and suggestions.\n",
      "* `recipe_finder`: a tool that finds relevant recipes based on user-provided ingredients.\n",
      "\n",
      "### Prompt Templates\n",
      "\n",
      "* These define the input formats and expected responses for the LangChain agents.\n",
      "\n",
      "### Integrations\n",
      "\n",
      "* FastMCP (Multi-Chain Protocol): a framework for building and deploying AI models.\n",
      "* Streamlit: a Python library for building web applications.\n",
      "* PyPDFLoader: a tool for loading and processing PDF files.\n",
      "* arxiv: a Python library for interacting with the arXiv research paper repository.\n",
      "\n",
      "The repository seems to be organized around various NLP tasks, including:\n",
      "\n",
      "1. **Research paper recommendation**: The `get_papers` tool fetches relevant research papers based on a user query.\n",
      "2. **Resume analysis**: The `analyze_resume` tool analyzes a user's resume and provides feedback and suggestions.\n",
      "3. **Recipe suggestion**: The `recipe_finder` tool finds relevant recipes based on user-provided ingredients.\n",
      "\n",
      "These tools and models are integrated using LangChain and FastMCP, allowing for the development of conversational AI applications.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
